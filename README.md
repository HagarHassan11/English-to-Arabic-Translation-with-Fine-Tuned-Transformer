# English-to-Arabic-Translation-with-Fine-Tuned-Transformer
Overview

This project fine-tunes a Transformer-based model for high-quality English-to-Arabic translation. Using MarianMT, a pre-trained sequence-to-sequence model, we enhance translation accuracy by training on the FLORES-200 dataset. The fine-tuning process leverages tokenization, sequence-to-sequence training, and evaluation metrics such as BLEU to optimize performance.

Key Features

✅ Fine-tuned MarianMT for English-to-Arabic translation

✅ Dataset: FLORES-200 for high-quality parallel data

✅ Tokenization & Preprocessing for optimized input handling

✅ Training Pipeline with Hugging Face Transformers & PyTorch

✅ Performance Evaluation using BLEU score
